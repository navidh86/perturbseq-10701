{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2586ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs example: ['GCGGCCGCGCGTGGTGGGGGAGGAGGGACCGGCGGCGCCCACGTGGCCTCCGCGGGCCCCGCCAGAGCCTGCGCCCGGGCCCTGACCGCACCTCTCGCCCCGCAGGACCATGGCCAACCTGGAGCGCACCTTCATCGCCATCAAGCCGGACGGCGTGCAGCGCGGCCTGGTGGGCGAGATCATCAAGCGCTTCGAGCAGAAGGGATTCCGCCTCGTGGCCATGAAGTTCCTCCGGGCCTCTGAAGAACACCTGAAGCAGCACTACATTGACCTGAAAGACCGACCATTCTTCCCTGGGCTGGTGAAGTACATGAACTCAGGGCCGGTTGTGGCCATGGTCTGGGAGGGGCTGAACGTGGTGAAGACAGGCCGAGTGATGCTTGGGGAGACCAATCCAGCAGATTCAAAGCCAGGCACCATTCGTGGGGACTTCTGCATTCAGGTTGGCAGGAACATCATTCATGGCAGTGATTCAGTAAAAAGTGCTGAAAAAGAAATCAGCCTATGGTTTAAGCCTGAAGAACTGGTTGACTACAAGTCTTGTGCTCATGACTGGGTCTATGAATAAGAGGTGGACACAACAGCAGTCTCCTTCAGCACGGCGTGGTGTGTCCCTGGACACAGCTCTTCATTCCATTGACTTAGAGGCAACAGGATTGATCATTCTTTTATAGAGCATATTTGCCAATAAAGCTTTTGGAAGCCGGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'TTAAGACTTCAGGCAGACAGATATGAACACATGAAGGGAGTAAAAACTCCAACCTCTGGACCACTTCCTGGAGTGCTCCTAACCCAGACTCCCACAGGGCTTCCTCCCATACTTCATTTAGGTCTTTGGCTCAAATGTTACCTCTTTAGAACTGCCTCAAGACTGGGGCAAGAGGAGCCCCAGTCTAGGGCTCTGAGCACCTGATTCTCTTCCGTCCATGTTCCTTTCCATAGGGCAAGAAATCCCACGGGCCAACGGGATGTACTCACTCAGAGCTCTTGCCCCTCTCCTTTTTAGAGCATACTGTGAGTTCCTGGGACCCTAGAAAATAGTGCCGCAATAGCAACAGGTCACCAGCTACAGGCTGCCTCAGCCTTTTCCCAAGGTCTATCCTCCTGAGGAGCAAGGGGTGGTCAAGGGGAGGCTATTTGCAGGTGTGGACAAAACTCTGATGTGCAGATGTCGTTGTTATAGGTGTGGGCAAAAGGCCATTTGAAGTATGGGACAGAGCTGGGGTgccaagagaagaaggaaggctGAGGGCCAAGGACATGTTCCCCCTCCAAAGCACTACATTCGAGGGCGTATCTCAGAAGGGTCCAAGAATCCTAAACTTAAATGTGGCCTTCCAGATTATGATGAAGGTATACATGTCAAGATAAAAGGATAGGAATTTTACATCACCGTttaatttgtagttttaaaatatgtagacaTGATATGTGGGCCTCCATTTGTACTACTGACTCAGTTCTTGCAAATATTAAGGGTAGACCTGTCCCTGATCTTCGTGTGTAAAATAGCACCTCCTCTGCCACTCTATTCTTACCCCAGTCTACATTTCTTCATAACATCGGTCACCATCTGATCTACATTTACTTATGTATCATCTCTTTACCCCCACTGGAATGTAAGTTCCACGTCCCTAGAGTCTATATTTTGTTTACTGCCTTATCCTTATACCTAAACAAGTGCCTGGCATGACACCACTCAGTAaaacttgaataaatgaataacaaatcTAGGCTCAAAGCTCTTACAGGTGAAAGGGGCTAACCTGGCAGTCACAAAGGAAAAATATCACTGTGACAACCGTCTGAGGAGCACCTTCTCCCCTTAAAATAACTCACTCAAAACAGTGGAGCTGCAAAGGTGACCAGAGAAAATCCATTAACACCTTAGTGCCCATGACACCAGGTGAGTTGATCGGGCTACTTGCGAGAGGGACATGTGAGTGAGCGTGGAGAGCTCTGAGTAGGCTGAAGAGCTGGATGTGGGGGATCTGGGCTGCTGGGAACCCCGAATGTTGGAATCGCCCTGACAACCTTATGAGGGTGGTCAGATATGGGTCCTGAGGAAACTGATTCCACGGCAACTGCCCGCCAACCCAGCACACCCCACAGTGAAGAGGGGCTTCTGATCCATGAGTAGGGCCCCTCAAATGATGTTCCCCTTTAGATGGTCATGGTCATCAGCAGGGGTGGGTGCATCGATCCTGTGGGGTCTTGGTGGTCGCAGACCAAGCGTCCCTCTTCTCACATCTAATTCACAACGTCTCATCGCTCCGCGGGAGCCCAGGAACTGCCCTACAACAGCCGAAATCCGGGTCGTCTATTCCAACGGACTCCTCTTCCGTTGGATACGGGGGCCGTGAGAGGCCAAACAACCTCCGCGAGGCTACCCCCGCAGCGACCGCTCGGCGGGCGCTCCATTCCTTTACCGGACTGTACACCTCGCCTCCTGGGGCCTGAGGCGGGAACCCGGGTCGCGCTGCTCCGAGGCCTCGGCGGACCGCGAAAGGATGGCTCACCACCCGGCCAGGGCCAGCCCCTGCAACAAGCTCAGCCCCGCCCGCTCCCGCGCCCAGGAGCGGCCGGCGGGGCGGGGTGGCTGGGCCGGCCTCGGGGGCGGGGGCTCGGGGGCGGGGGCTCAGGGGCGGGGCCACGTGGCTCCTCCTTGCGGGCGGAGGCAGGCGGTGCCGCGGCGCCGGGACCCGACTCATCCGGTGCTTGCGTGTGGTGGTGAGCGCAGCGCCGAGGATGAGGAGGTGCAACAGCGGCTCCGGGCCGCCGCCgtcgctgctgctgctgctgctgtggctgctCGCGGTTCCCGGCGCTAACGCGGCCCCGCGGTCGGCGCTCTATTCGCCTTCCGACCCGCTGACGCTGCTGCAGGCGGACACGGTGCGCGGCGCGGTGCTGGGCTCCCGCAGCGCCTGGGCCGTGGAGTTCTTCGCCTCCTGGTGCGGCCACTGCATCGCCTTCGCCCCGACGTGGAAGGCGCTGGCCGAAGACGTCAAAGGTGAGAAGCGGGGGCGGCCCGCTCCCCCGTGCCCCGCCGCCCGGACCCCTCCACCTGCCCGGTGGGCAGCCTCCTACTCCGGGAGCGCGTCCCTCTTCCAGTCACCTCCGCCCACCTCTTCCTCTCCGCGCATCCCACGCCCACCACCTTCATGTTTCTCAGCTCCATTTTTGCCTCCCGCTCCCTTGCCTTTTCCTCTTGGCCCTCATCCCGAGCTTCTTTCCCTCTCCGCGGCCTGGGCCCCCAGACCTGCCCTGCTCGCCCGGTTCGTCCCCCGCCAGGTTACTGCCTCCTTCCCTGCGAGAGGtctcctttgttccttcctccGCATGGTCCGCTGACACCTTACCGTTTCCTGGGCCAGCCTTGTGGCAGTCCTCTTCCTCTCGTCCAGGGACCCATGTCATCTTCCTGCCAGGTTCCCTAGCTGCTGTGGGTCAGTgctggggagaaaagggaacggACATCCATTTTGCCTCCCTGCCACCTTTCTCCACTCCCTCGACTGGTTGACCAGGTGGTACATAgacttcctttctcatttttgtctTTGATGTGGGTATGTCCACCTGGCAGTTTTCCTATCTCTGTTCCGGGCCTCTCCTGACCTGCCGCCTCCAGCTCCTTAGCTTCTGGGCCacatccttctctctcttcccccgcTCAGACCCTTATATTTTAAACCCATGCACCCAGAGGCCTTTGGTGCGTGTTTGGCTCTTGCTCCATGTTCCCAATCCCTGGTTGGGGCACCAAAACTATATTTCCTGAATTCCAAGACACCATCAACTGTGAGACACACTGTTCATTTTATGACAGCCCAAGGGGTAGAAGGATAGGAAAAAACCACTACATGTATAAAGATTTGAAGGTGAACTCTATTACAGAAATGCTAAAATTTTGAAACACTTAAGACTTAGAGTCGAGGAAATATggtaataatagtaattaatatTTACGGAGCACCTATCATTTCAGGCACCTTACTAAGTACTCGGCATACATTGTCTCATAGTTTCCTTTCCACAATTTAAAGATTCCTGTGGTGTCACACATCCACCTAATCCAGGTAGTGGGTATTTGGCGGGCAGGAGTGGTATAGTGATAGTGATTTGTTCCTGACTTGCTGCCTCTGAATGGAGTTACTTCCTGACTCTGGTGCTAGGTCGCTAGCACGGCCTCCTACCCTGAAATGGTCATTGAATGAGAACTGTGGGCATCTTTGGAAATAGGCCTTTTTCTAGAGAAAGGGGAACTGGAACCATTGTTCCACTGTTGGTATTTCTTGATAAATTCAACTCTTTGTGGTAGAACAGTGCCTTTGTAGATGTTAGTTTTCAGACTCACCCTCCTCTTTTCCTGAGCAGCTTGCAGATTGGTTTCTGAAGAGGCGAGATGGAACCATGTGTACATCTTTTAGCCATATGGtcctggaaaattatttttaaatgaaacttatAAGCTGTTTTGACCTCCAGTTTACACTTGCCTACATGCCTCACTGGTCCCATTAGCTGAACGTGCCTTTCTGAGCAGTGCTTCACACCATTTCAACACATTTGGTCAGGGGAGGAGTGATGTCAACACATCTGgtcaggggaggagggaggaagaacatGGATCATGGAACTGGAACACCTAAGATAAGGTAATCTCAGGGGACATTGTGGGCATGAACATCATAAAAATTCCCAGA']\n",
      "Label example: tensor(-0.1313)\n"
     ]
    }
   ],
   "source": [
    "from reference_data_alternate import (\n",
    "    PairPerturbSeqDataset,\n",
    "    perturbseq_collate,\n",
    "    get_dataloader\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = get_dataloader(type=\"train\", batch_size=4)\n",
    "batch_inputs, batch_labels = next(iter(train_loader))\n",
    "\n",
    "print(\"Inputs example:\", batch_inputs[0])\n",
    "print(\"Label example:\", batch_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc7b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean: -0.022736955\n",
      "Training std: 0.15207928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_dataset = PairPerturbSeqDataset(type=\"train\")\n",
    "all_y = []\n",
    "\n",
    "for _, y in DataLoader(train_dataset, batch_size=512):\n",
    "    all_y.extend(y.numpy())\n",
    "\n",
    "all_y = np.array(all_y)\n",
    "mu = all_y.mean()\n",
    "sigma = all_y.std()\n",
    "\n",
    "print(\"Training mean:\", mu)\n",
    "print(\"Training std:\", sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a32beb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def weighted_mse_loss(pred, target, mu, sigma, alpha=3.0, threshold=1.0):\n",
    "    mse = (pred - target)**2\n",
    "    z = (target - mu) / sigma\n",
    "    weights = torch.where(torch.abs(z) > threshold,\n",
    "                          torch.tensor(alpha, device=target.device),\n",
    "                          torch.tensor(1.0, device=target.device))\n",
    "    return (weights * mse).sum() / weights.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734caafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tf_seqs = pickle.load(open(\"tf_sequences.pkl\", \"rb\"))\n",
    "gene_seqs = pickle.load(open(\"gene_sequences_4000bp.pkl\", \"rb\"))\n",
    "\n",
    "tf_to_id = {tf: i for i, tf in enumerate(tf_seqs.keys())}\n",
    "gene_to_id = {gene: i for i, gene in enumerate(gene_seqs.keys())}\n",
    "\n",
    "num_tfs = len(tf_to_id)\n",
    "num_genes = len(gene_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ab6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGeneIDModel(nn.Module):\n",
    "    def __init__(self, num_tfs, num_genes, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.tf_emb = nn.Embedding(num_tfs, emb_dim)\n",
    "        self.gene_emb = nn.Embedding(num_genes, emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tf_ids, gene_ids):\n",
    "        t = self.tf_emb(tf_ids)\n",
    "        g = self.gene_emb(gene_ids)\n",
    "        h = torch.cat([t, g], dim=-1)\n",
    "        return self.mlp(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c66c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_map = {\"A\":0, \"C\":1, \"G\":2, \"T\":3, \"N\":0}\n",
    "\n",
    "def encode_seq(seq):\n",
    "    return torch.tensor([dna_map.get(ch, 0) for ch in seq], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0c014c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab=4, emb_dim=16, hidden=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, emb_dim)\n",
    "        self.conv = nn.Conv1d(emb_dim, hidden, kernel_size=7, padding=3)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "    def forward(self, seq_batch):\n",
    "        # seq_batch: list of 1D LongTensors (variable length)\n",
    "        padded = nn.utils.rnn.pad_sequence(seq_batch, batch_first=True)\n",
    "        x = self.emb(padded)                     # (B, L, emb)\n",
    "        x = x.permute(0,2,1)                     # (B, emb, L)\n",
    "        x = torch.relu(self.conv(x))             # (B, hidden, L)\n",
    "        x = self.pool(x).squeeze(-1)             # (B, hidden)\n",
    "        return x\n",
    "\n",
    "class SeqBiEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleSeqEncoder()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),   # because encoder outputs 64-d each\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tf_seqs, gene_seqs):\n",
    "        tf_h = self.encoder(tf_seqs)\n",
    "        gene_h = self.encoder(gene_seqs)\n",
    "        h = torch.cat([tf_h, gene_h], dim=-1)\n",
    "        return self.mlp(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old prepare_id_batch function\n",
    "\n",
    "def prepare_id_batch(batch_inputs):\n",
    "    tf_ids = []\n",
    "    gene_ids = []\n",
    "    for tf_seq, gene_seq in batch_inputs:\n",
    "        # keys are exactly those in your parquet\n",
    "        tf_name = tf_seq   # since the dataset returns raw string seq, get name via reverse lookup\n",
    "        gene_name = gene_seq\n",
    "        tf_ids.append(tf_to_id[tf_name])\n",
    "        gene_ids.append(gene_to_id[gene_name])\n",
    "    return torch.tensor(tf_ids), torch.tensor(gene_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba58f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old training loop for sequence model\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# model = SeqBiEncoder().cuda()\n",
    "model = SeqBiEncoder()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        \n",
    "        # encode sequences\n",
    "        tf_seqs = [encode_seq(tf) for tf, gene in batch_inputs]\n",
    "        gene_seqs = [encode_seq(gene) for tf, gene in batch_inputs]\n",
    "\n",
    "        # tf_seqs = [t.cuda() for t in tf_seqs]\n",
    "        # gene_seqs = [g.cuda() for g in gene_seqs]\n",
    "        # y = batch_labels.cuda()\n",
    "        y= batch_labels\n",
    "\n",
    "        preds = model(tf_seqs, gene_seqs)\n",
    "\n",
    "        loss = weighted_mse_loss(preds, y, mu, sigma)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch\", epoch, \"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab01c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New loop with option for ID only model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_one_epoch(model, loader, mu, sigma, optimizer, use_sequences=True, device=\"cuda\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # ------------------------\n",
    "        # 1. Prepare TF and gene IDs\n",
    "        # ------------------------\n",
    "        tf_ids = torch.tensor([tf_to_id[x[\"tf_name\"]] for x in batch_x], dtype=torch.long).to(device)\n",
    "        gene_ids = torch.tensor([gene_to_id[x[\"gene_name\"]] for x in batch_x], dtype=torch.long).to(device)\n",
    "\n",
    "        # ------------------------\n",
    "        # 2. Prepare sequences (optional)\n",
    "        # ------------------------\n",
    "        if use_sequences:\n",
    "            tf_seqs = [encode_seq(x[\"tf_seq\"]).to(device) for x in batch_x]\n",
    "            gene_seqs = [encode_seq(x[\"gene_seq\"]).to(device) for x in batch_x]\n",
    "        else:\n",
    "            tf_seqs, gene_seqs = None, None\n",
    "\n",
    "        # ------------------------\n",
    "        # 3. Forward pass\n",
    "        # ------------------------\n",
    "        if use_sequences:\n",
    "            preds = model(tf_seqs, gene_seqs)          # sequence model\n",
    "        else:\n",
    "            preds = model(tf_ids, gene_ids)            # ID baseline model\n",
    "\n",
    "        # ------------------------\n",
    "        # 4. Weighted MSE Loss\n",
    "        # ------------------------\n",
    "        z = (batch_y - mu) / sigma\n",
    "        weights = torch.where(torch.abs(z) > 1.0, torch.tensor(3.0, device=device), torch.tensor(1.0, device=device))\n",
    "        loss = ((preds - batch_y) ** 2 * weights).sum() / weights.sum()\n",
    "\n",
    "        # ------------------------\n",
    "        # 5. Backprop\n",
    "        # ------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(batch_y)\n",
    "        count += len(batch_y)\n",
    "\n",
    "    return total_loss / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID only model training, baseline\n",
    "\n",
    "model = TFGeneIDModel(num_tfs, num_genes).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = train_one_epoch(model, train_loader, mu, sigma, optimizer,\n",
    "                           use_sequences=False, device=\"cuda\")\n",
    "    print(f\"Epoch {epoch} | Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c263b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence-based model training\n",
    "\n",
    "model = SeqBiEncoder().to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = train_one_epoch(model, train_loader, mu, sigma, optimizer,\n",
    "                           use_sequences=True, device=\"cuda\")\n",
    "    print(f\"Epoch {epoch} | Loss = {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMU_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
