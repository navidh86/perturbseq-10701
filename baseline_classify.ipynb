{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navidh86/perturbseq-10701/blob/master/baseline_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2e5d23ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e5d23ad",
        "outputId": "fa0e2543-e241-4d19-d77a-1898fffde35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'perturbseq-10701'...\n",
            "remote: Enumerating objects: 201, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 201 (delta 39), reused 45 (delta 16), pack-reused 121 (from 2)\u001b[K\n",
            "Receiving objects: 100% (201/201), 260.57 MiB | 12.74 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "Updating files: 100% (53/53), done.\n",
            "/content/perturbseq-10701\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastparquet) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
            "Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastparquet\n",
            "Successfully installed fastparquet-2024.11.0\n"
          ]
        }
      ],
      "source": [
        "# ONLY FOR COLAB\n",
        "!git clone https://github.com/navidh86/perturbseq-10701.git\n",
        "%cd ./perturbseq-10701\n",
        "!pip install fastparquet tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and device\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data.reference_data_classification import get_dataloader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VptI-6ncTr-5",
        "outputId": "8bbad7a8-dace-4848-f98a-3988396bfa01"
      },
      "id": "VptI-6ncTr-5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8b6f32b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b6f32b1",
        "outputId": "5c0aa984-4fe2-4d16-c9a9-cc39c10cbcdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 10845\n",
            "Test size : 2325\n"
          ]
        }
      ],
      "source": [
        "# # Create dataloaders (point to data/ paths explicitly)\n",
        "# train_loader = get_dataloader(\n",
        "#     parquet_path='data/tf_gene_expression_labeled.parquet',\n",
        "#     tf_sequences_path='data/tf_sequences.pkl',\n",
        "#     gene_sequences_path='data/gene_sequences_4000bp.pkl',\n",
        "#     batch_size=128,\n",
        "#     type='train',\n",
        "#     majority_fraction=0.01\n",
        "# )\n",
        "# test_loader = get_dataloader(\n",
        "#     parquet_path='data/tf_gene_expression_labeled.parquet',\n",
        "#     tf_sequences_path='data/tf_sequences.pkl',\n",
        "#     gene_sequences_path='data/gene_sequences_4000bp.pkl',\n",
        "#     batch_size=256,\n",
        "#     type='test',\n",
        "#     majority_fraction=0.01\n",
        "# )\n",
        "\n",
        "# print('Train size:', len(train_loader.dataset))\n",
        "# print('Test size :', len(test_loader.dataset))\n",
        "\n",
        "# Create dataloaders (point to data/ paths explicitly)\n",
        "train_loader = get_dataloader(\n",
        "    parquet_path='data/tf_gene_expression_labeled_v2.parquet',\n",
        "    tf_sequences_path='data/tf_sequences.pkl',\n",
        "    gene_sequences_path='data/gene_sequences_4000bp.pkl',\n",
        "    batch_size=128,\n",
        "    type='train',\n",
        "    majority_fraction=0.005\n",
        ")\n",
        "test_loader = get_dataloader(\n",
        "    parquet_path='data/tf_gene_expression_labeled_v2.parquet',\n",
        "    tf_sequences_path='data/tf_sequences.pkl',\n",
        "    gene_sequences_path='data/gene_sequences_4000bp.pkl',\n",
        "    batch_size=256,\n",
        "    type='test',\n",
        "    majority_fraction=0.005\n",
        ")\n",
        "\n",
        "print('Train size:', len(train_loader.dataset))\n",
        "print('Test size :', len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "15c9f72b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15c9f72b",
        "outputId": "b05c6e75-fdca-47f9-f201-d29ce719bd9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique TFs (combined): 223\n",
            "Unique Genes (combined): 4305\n",
            "Num classes: 3\n"
          ]
        }
      ],
      "source": [
        "train_ds = train_loader.dataset\n",
        "test_ds = test_loader.dataset\n",
        "combined_df = pd.concat([train_ds.df, test_ds.df]).reset_index(drop=True)\n",
        "\n",
        "# unique names from combined set\n",
        "tf_names = combined_df['tf_name'].unique().tolist()\n",
        "gene_names = combined_df['gene_name'].unique().tolist()\n",
        "\n",
        "# create mappings\n",
        "tf_to_id = {n: i for i, n in enumerate(tf_names)}\n",
        "gene_to_id = {n: i for i, n in enumerate(gene_names)}\n",
        "\n",
        "num_tfs = len(tf_to_id)\n",
        "num_genes = len(gene_to_id)\n",
        "# Use classes from training split\n",
        "num_classes = len(train_ds.df['expression_label'].unique())\n",
        "\n",
        "print('Unique TFs (combined):', num_tfs)\n",
        "print('Unique Genes (combined):', num_genes)\n",
        "print('Num classes:', num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cached embeddings and prepare name lists\n",
        "import pickle\n",
        "\n",
        "# Load cached TF/gene embeddings produced by the embedding notebook\n",
        "tf_embed_cache = pickle.load(open('embeds/tf_cls.pkl', 'rb'))\n",
        "gene_embed_cache = pickle.load(open('embeds/gn_cls.pkl', 'rb'))\n",
        "\n",
        "# Convert any numpy arrays to torch tensors\n",
        "for k in list(tf_embed_cache.keys()):\n",
        "    v = tf_embed_cache[k]\n",
        "    if not isinstance(v, torch.Tensor):\n",
        "        tf_embed_cache[k] = torch.tensor(v, dtype=torch.float32)\n",
        "for k in list(gene_embed_cache.keys()):\n",
        "    v = gene_embed_cache[k]\n",
        "    if not isinstance(v, torch.Tensor):\n",
        "        gene_embed_cache[k] = torch.tensor(v, dtype=torch.float32)\n",
        "\n",
        "# Expose name lists and counts (from the caches to ensure consistent mapping)\n",
        "tf_names = list(tf_embed_cache.keys())\n",
        "gene_names = list(gene_embed_cache.keys())\n",
        "num_tfs = len(tf_names)\n",
        "num_genes = len(gene_names)\n",
        "# number of classes is taken from the training split\n",
        "num_classes = len(train_loader.dataset.df['expression_label'].unique())\n",
        "\n",
        "print('TFs in cache:', num_tfs)\n",
        "print('Genes in cache:', num_genes)\n",
        "print('Num classes:', num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn0lV1UaWo7K",
        "outputId": "08cc5817-8bdc-477f-9339-2a014e45c06d"
      },
      "id": "Fn0lV1UaWo7K",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFs in cache: 223\n",
            "Genes in cache: 5307\n",
            "Num classes: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "af14d080",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af14d080",
        "outputId": "b53367c6-965e-4605-af00-880cb00bc439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFGeneIDModel(\n",
            "  (tf_emb): Embedding(223, 64)\n",
            "  (gene_emb): Embedding(5307, 64)\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=128, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# ID-only model definition\n",
        "class TFGeneIDModel(nn.Module):\n",
        "    def __init__(self, num_tfs, num_genes, emb_dim=64, hidden_dim=256, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.tf_emb = nn.Embedding(num_tfs, emb_dim)\n",
        "        self.gene_emb = nn.Embedding(num_genes, emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2*emb_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, tf_ids, gene_ids):\n",
        "        t = self.tf_emb(tf_ids)\n",
        "        g = self.gene_emb(gene_ids)\n",
        "        h = torch.cat([t, g], dim=-1)\n",
        "        return self.mlp(h)\n",
        "\n",
        "# Instantiate model\n",
        "model = TFGeneIDModel(num_tfs=num_tfs, num_genes=num_genes, emb_dim=64, hidden_dim=256, num_classes=num_classes).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "87abaa78",
      "metadata": {
        "id": "87abaa78"
      },
      "outputs": [],
      "source": [
        "# Training / evaluation helpers\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def prepare_id_batch(batch_x, device=device):\n",
        "    tf_ids = torch.tensor([tf_to_id[item['tf_name']] for item in batch_x], dtype=torch.long, device=device)\n",
        "    gene_ids = torch.tensor([gene_to_id[item['gene_name']] for item in batch_x], dtype=torch.long, device=device)\n",
        "    return tf_ids, gene_ids\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    pbar = tqdm(loader)\n",
        "    for batch_x, batch_y in pbar:\n",
        "        batch_y = batch_y.to(device)\n",
        "        tf_ids, gene_ids = prepare_id_batch(batch_x)\n",
        "\n",
        "        logits = model(tf_ids, gene_ids)\n",
        "        loss = loss_fn(logits, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == batch_y).sum().item()\n",
        "        total += len(batch_y)\n",
        "        total_loss += loss.item() * len(batch_y)\n",
        "        pbar.set_postfix({'loss': total_loss/total, 'acc': total_correct/total})\n",
        "\n",
        "    return total_loss / total, total_correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_y = batch_y.to(device)\n",
        "        tf_ids, gene_ids = prepare_id_batch(batch_x)\n",
        "        logits = model(tf_ids, gene_ids)\n",
        "        loss = loss_fn(logits, batch_y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == batch_y).sum().item()\n",
        "        total += len(batch_y)\n",
        "        total_loss += loss.item() * len(batch_y)\n",
        "    return total_loss/total, total_correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "399d02d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "399d02d1",
        "outputId": "af5524d6-8b11-437b-94f2-02539d711ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 53.02it/s, loss=0.964, acc=0.509]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.9643, Train Acc: 0.5087 | Val Loss: 0.8636, Val Acc: 0.5712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 87.76it/s, loss=0.789, acc=0.627]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss: 0.7893, Train Acc: 0.6275 | Val Loss: 0.7875, Val Acc: 0.6284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 79.78it/s, loss=0.706, acc=0.677]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss: 0.7057, Train Acc: 0.6768 | Val Loss: 0.7632, Val Acc: 0.6353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 79.16it/s, loss=0.64, acc=0.715]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss: 0.6404, Train Acc: 0.7145 | Val Loss: 0.7562, Val Acc: 0.6477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 89.74it/s, loss=0.57, acc=0.75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss: 0.5704, Train Acc: 0.7502 | Val Loss: 0.7402, Val Acc: 0.6675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 89.13it/s, loss=0.506, acc=0.784]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss: 0.5062, Train Acc: 0.7841 | Val Loss: 0.7705, Val Acc: 0.6723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 87.37it/s, loss=0.451, acc=0.809]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss: 0.4514, Train Acc: 0.8094 | Val Loss: 0.7779, Val Acc: 0.6757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 95.42it/s, loss=0.405, acc=0.836]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss: 0.4046, Train Acc: 0.8357 | Val Loss: 0.7867, Val Acc: 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 91.40it/s, loss=0.351, acc=0.859]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train Loss: 0.3512, Train Acc: 0.8590 | Val Loss: 0.8354, Val Acc: 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 96.13it/s, loss=0.314, acc=0.872]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.3142, Train Acc: 0.8717 | Val Loss: 0.8800, Val Acc: 0.6834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 95.43it/s, loss=0.292, acc=0.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Loss: 0.2923, Train Acc: 0.8803 | Val Loss: 0.8874, Val Acc: 0.6938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 94.49it/s, loss=0.255, acc=0.899]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Loss: 0.2550, Train Acc: 0.8995 | Val Loss: 0.9664, Val Acc: 0.6826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 79.24it/s, loss=0.234, acc=0.906]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Loss: 0.2336, Train Acc: 0.9062 | Val Loss: 0.9848, Val Acc: 0.6865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 79.98it/s, loss=0.196, acc=0.922]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: 0.1958, Train Acc: 0.9221 | Val Loss: 1.0193, Val Acc: 0.6890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 79.74it/s, loss=0.181, acc=0.931]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: 0.1812, Train Acc: 0.9314 | Val Loss: 1.0681, Val Acc: 0.6877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 92.61it/s, loss=0.165, acc=0.938]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: 0.1653, Train Acc: 0.9382 | Val Loss: 1.1293, Val Acc: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 90.65it/s, loss=0.145, acc=0.945]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: 0.1454, Train Acc: 0.9449 | Val Loss: 1.1634, Val Acc: 0.6895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 90.30it/s, loss=0.14, acc=0.946]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: 0.1398, Train Acc: 0.9462 | Val Loss: 1.2269, Val Acc: 0.6873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 95.07it/s, loss=0.125, acc=0.953]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: 0.1253, Train Acc: 0.9528 | Val Loss: 1.2610, Val Acc: 0.6942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 81.91it/s, loss=0.112, acc=0.958]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: 0.1117, Train Acc: 0.9583 | Val Loss: 1.2940, Val Acc: 0.6946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 94.09it/s, loss=0.0989, acc=0.963]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | Train Loss: 0.0989, Train Acc: 0.9626 | Val Loss: 1.3681, Val Acc: 0.6882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 89.27it/s, loss=0.0946, acc=0.965]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | Train Loss: 0.0946, Train Acc: 0.9647 | Val Loss: 1.3569, Val Acc: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 93.05it/s, loss=0.0844, acc=0.97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | Train Loss: 0.0844, Train Acc: 0.9700 | Val Loss: 1.4341, Val Acc: 0.6933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 85.89it/s, loss=0.0765, acc=0.972]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | Train Loss: 0.0765, Train Acc: 0.9719 | Val Loss: 1.4742, Val Acc: 0.6972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 91.15it/s, loss=0.0808, acc=0.971]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | Train Loss: 0.0808, Train Acc: 0.9708 | Val Loss: 1.4711, Val Acc: 0.6925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:01<00:00, 84.35it/s, loss=0.0673, acc=0.976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 | Train Loss: 0.0673, Train Acc: 0.9762 | Val Loss: 1.5427, Val Acc: 0.7002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 87.29it/s, loss=0.0686, acc=0.975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 | Train Loss: 0.0686, Train Acc: 0.9747 | Val Loss: 1.5450, Val Acc: 0.6920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 90.52it/s, loss=0.0575, acc=0.978]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 | Train Loss: 0.0575, Train Acc: 0.9782 | Val Loss: 1.5907, Val Acc: 0.6933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 90.62it/s, loss=0.0567, acc=0.978]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 | Train Loss: 0.0567, Train Acc: 0.9782 | Val Loss: 1.6480, Val Acc: 0.6959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 87.88it/s, loss=0.0573, acc=0.98]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 | Train Loss: 0.0573, Train Acc: 0.9805 | Val Loss: 1.6543, Val Acc: 0.6920\n",
            "Saved model to models/tf_gene_id_baseline.pt\n"
          ]
        }
      ],
      "source": [
        "# Train for a few epochs\n",
        "num_epochs = 30\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch:02d} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "# Save model\n",
        "os.makedirs('models', exist_ok=True)\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'tf_to_id': tf_to_id,\n",
        "    'gene_to_id': gene_to_id\n",
        "}, 'models/tf_gene_id_baseline.pt')\n",
        "print('Saved model to models/tf_gene_id_baseline.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation (nt_classify-style): compute loss, accuracy, macro-F1, and classification report\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import json\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model_ntstyle(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # support both cache-based and id-based models\n",
        "        try:\n",
        "            logits = model(batch_x)\n",
        "        except TypeError:\n",
        "            tf_ids, gene_ids = prepare_id_batch(batch_x, device=device)\n",
        "            logits = model(tf_ids, gene_ids)\n",
        "\n",
        "        loss = loss_fn(logits, batch_y)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        total_loss += loss.item() * len(batch_y)\n",
        "        total_correct += (preds == batch_y).sum().item()\n",
        "        total_samples += len(batch_y)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    return avg_loss, accuracy, macro_f1, all_labels, all_preds\n",
        "\n",
        "# Run final eval and print same outputs as nt_classify\n",
        "test_loss, test_acc, test_f1, y_true, y_pred = eval_model_ntstyle(model, test_loader)\n",
        "\n",
        "print(\"Final Test Accuracy:\", test_acc)\n",
        "print(\"Final Test Macro F1:\", test_f1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "# Save same metrics JSON for reproducibility\n",
        "os.makedirs('results', exist_ok=True)\n",
        "metrics = {\n",
        "    'test_loss': float(test_loss),\n",
        "    'accuracy': float(test_acc),\n",
        "    'macro_f1': float(test_f1),\n",
        "    'classification_report': classification_report(y_true, y_pred, digits=4),\n",
        "}\n",
        "with open('results/baseline_classify_metrics_ntstyle.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print('\\nSaved metrics to results/baseline_classify_metrics_ntstyle.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_orkTC3RljZ",
        "outputId": "3bf627e5-de51-4bdb-a0a1-0c0e06035f2f"
      },
      "id": "y_orkTC3RljZ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.6920430107526881\n",
            "Final Test Macro F1: 0.6960908410748012\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6902    0.6514    0.6702       766\n",
            "           1     0.7966    0.7715    0.7839       731\n",
            "           2     0.6107    0.6594    0.6341       828\n",
            "\n",
            "    accuracy                         0.6920      2325\n",
            "   macro avg     0.6992    0.6941    0.6961      2325\n",
            "weighted avg     0.6954    0.6920    0.6931      2325\n",
            "\n",
            "\n",
            "Saved metrics to results/baseline_classify_metrics_ntstyle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvHPJ77iXYeE"
      },
      "id": "xvHPJ77iXYeE",
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}