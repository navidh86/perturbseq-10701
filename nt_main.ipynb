{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navidh86/perturbseq-10701/blob/master/nt_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a28f2766",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a28f2766",
        "outputId": "c6150375-79f1-4557-c22e-ae346480bb14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'perturbseq-10701'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 105 (delta 7), reused 8 (delta 4), pack-reused 91 (from 1)\u001b[K\n",
            "Receiving objects: 100% (105/105), 115.03 MiB | 35.55 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "Updating files: 100% (31/31), done.\n",
            "/content/perturbseq-10701\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastparquet) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
            "Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastparquet\n",
            "Successfully installed fastparquet-2024.11.0\n"
          ]
        }
      ],
      "source": [
        "# ONLY FOR COLAB\n",
        "!git clone https://github.com/navidh86/perturbseq-10701.git\n",
        "%cd ./perturbseq-10701\n",
        "!pip install fastparquet tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8vhUkZG6au3",
        "outputId": "0939ec37-7ad6-48ec-e1bf-4c74b19d9e58"
      },
      "id": "X8vhUkZG6au3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-u1ibqv1p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-u1ibqv1p\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 2a61590a479d3b1f77059f75caee7cc22760019d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
            "Collecting huggingface-hub<2.0,>=1.0.0 (from transformers==5.0.0.dev0)\n",
            "  Downloading huggingface_hub-1.1.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (0.22.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (0.20.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (2025.11.12)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers==5.0.0.dev0) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers==5.0.0.dev0) (1.3.1)\n",
            "Downloading huggingface_hub-1.1.6-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.1/516.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-5.0.0.dev0-py3-none-any.whl size=10783992 sha256=2dcfdbd0a6c7b18cf56563b6b8ea2c08860ce83a141b79158db3cf099c2c7f73\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2maioczq/wheels/54/cb/3f/83103de5575c534436d6a4686686dead458238dfaf1147e98d\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.1.6 transformers-5.0.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e18d8938",
      "metadata": {
        "id": "e18d8938"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataloader\n",
        "from reference_data_alternate import (\n",
        "    PairPerturbSeqDataset,\n",
        "    perturbseq_collate,\n",
        "    get_dataloader\n",
        ")\n",
        "\n",
        "train_loader = get_dataloader(type=\"train\", batch_size=4)   # small batch for NT\n",
        "test_loader  = get_dataloader(type=\"test\", batch_size=4)\n",
        "\n",
        "print(\"Train size:\", len(train_loader))\n",
        "print(\"Test size: \", len(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh-DeZYn9BLD",
        "outputId": "4c634ea5-f939-42dd-8cfc-7b113586540f"
      },
      "id": "lh-DeZYn9BLD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 236692\n",
            "Test size:  59174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calc summary stats\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = PairPerturbSeqDataset(type=\"train\")\n",
        "loader = DataLoader(train_dataset, batch_size=512, collate_fn=perturbseq_collate)\n",
        "\n",
        "all_y = []\n",
        "for _, y in loader:\n",
        "    all_y.extend(y.numpy())\n",
        "\n",
        "all_y = np.array(all_y)\n",
        "mu = all_y.mean()\n",
        "sigma = all_y.std()\n",
        "\n",
        "print(\"mu =\", mu)\n",
        "print(\"sigma =\", sigma)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5UtATY-9S84",
        "outputId": "3054cd1f-7f5e-451d-aa03-10c94a9d03ee"
      },
      "id": "C5UtATY-9S84",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu = -0.022736955\n",
            "sigma = 0.15207928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss fucntion\n",
        "def weighted_mse_loss(pred, target, mu, sigma, alpha=3.0, threshold=1.0):\n",
        "    z = (target - mu) / sigma\n",
        "    weights = torch.where(\n",
        "        torch.abs(z) > threshold,\n",
        "        torch.tensor(alpha, device=target.device),\n",
        "        torch.tensor(1.0, device=target.device)\n",
        "    )\n",
        "    mse = (pred - target)**2\n",
        "    return (weights * mse).sum() / weights.sum()\n"
      ],
      "metadata": {
        "id": "Npzpmunz9VN4"
      },
      "id": "Npzpmunz9VN4",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /root/.cache/huggingface/transformers\n",
        "# !rm -rf /root/.cache/huggingface/hub\n",
        "\n",
        "# !pip install --upgrade git+https://github.com/huggingface/transformers.git\n"
      ],
      "metadata": {
        "id": "C_03HLs8_8lI"
      },
      "id": "C_03HLs8_8lI",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from transformers import AutoTokenizer\n",
        "# tok = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")\n",
        "# print(type(tok))\n",
        "# print(tok.__class__.__name__)\n",
        "# print(\"batch_encode_plus\" in dir(tok))\n"
      ],
      "metadata": {
        "id": "CfKykuvEBZlp"
      },
      "id": "CfKykuvEBZlp",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OLD\n"
      ],
      "metadata": {
        "id": "pN9cW12-AL0e"
      },
      "id": "pN9cW12-AL0e"
    },
    {
      "cell_type": "code",
      "source": [
        "# # NT ENCODER\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# class NTEncoder(nn.Module):\n",
        "#     def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", device=\"cuda\"):\n",
        "#         super().__init__()\n",
        "#         self.device = device\n",
        "\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#         self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "#         self.model.eval()\n",
        "\n",
        "#         self.max_len = self.tokenizer.model_max_length\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def forward(self, seq: str):\n",
        "#         seq = seq.upper().replace(\"U\", \"T\")\n",
        "\n",
        "#         # chunk the sequence\n",
        "#         chunks = [seq[i:i+self.max_len] for i in range(0, len(seq), self.max_len)]\n",
        "#         chunk_embs = []\n",
        "\n",
        "#         for chunk in chunks:\n",
        "#             # Works with all HF models including ESM\n",
        "#             tokens = self.tokenizer(\n",
        "#                 [chunk],\n",
        "#                 return_tensors=\"pt\",\n",
        "#                 padding=\"max_length\",\n",
        "#                 max_length=self.max_len,\n",
        "#                 truncation=True\n",
        "#             ).to(self.device)\n",
        "\n",
        "#             input_ids = tokens[\"input_ids\"]\n",
        "#             attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "#             out = self.model(\n",
        "#                 input_ids,\n",
        "#                 attention_mask=attention_mask,\n",
        "#                 encoder_attention_mask=attention_mask,\n",
        "#                 output_hidden_states=True\n",
        "#             )\n",
        "\n",
        "#             # ⭐ FIX: use hidden_states correctly\n",
        "#             hidden = out.hidden_states[-1].squeeze(0)   # (L, D)\n",
        "#             attn = attention_mask.squeeze(0).unsqueeze(-1)  # (L, 1)\n",
        "\n",
        "#             # mean over non-pad tokens\n",
        "#             embed = (hidden * attn).sum(0) / attn.sum()\n",
        "#             chunk_embs.append(embed)\n",
        "\n",
        "#         return torch.stack(chunk_embs).mean(0)\n"
      ],
      "metadata": {
        "id": "ny5WR5IJ69mv"
      },
      "id": "ny5WR5IJ69mv",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Biencoder model\n",
        "# class NTBiEncoder(nn.Module):\n",
        "#     def __init__(self, encoder, emb_dim=1280):\n",
        "#         super().__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.mlp = nn.Sequential(\n",
        "#             nn.Linear(emb_dim * 2, 512),   # 1280*2 = 2560\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, 1)\n",
        "#         )\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def encode(self, seq):\n",
        "#         return self.encoder(seq)\n",
        "\n",
        "#     def forward(self, tf_seqs, gene_seqs):\n",
        "#         tf_embs = []\n",
        "#         gene_embs = []\n",
        "#         for tf, gene in zip(tf_seqs, gene_seqs):\n",
        "#             tf_embs.append(self.encoder(tf))\n",
        "#             gene_embs.append(self.encoder(gene))\n",
        "\n",
        "#         tf_embs = torch.stack(tf_embs)\n",
        "#         gene_embs = torch.stack(gene_embs)\n",
        "#         h = torch.cat([tf_embs, gene_embs], dim=-1)\n",
        "#         return self.mlp(h).squeeze(-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "iEdOYdlu8ewA"
      },
      "id": "iEdOYdlu8ewA",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NEW"
      ],
      "metadata": {
        "id": "gw5wADXRA2nq"
      },
      "id": "gw5wADXRA2nq"
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "class NTEncoderCLS(nn.Module):\n",
        "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.max_len = self.tokenizer.model_max_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, seq: str):\n",
        "        seq = seq.upper().replace(\"U\", \"T\")\n",
        "        embeds = []\n",
        "\n",
        "        chunks = [seq[i:i+self.max_len] for i in range(0, len(seq), self.max_len)]\n",
        "\n",
        "        for chunk in chunks:\n",
        "            tokens = self.tokenizer(\n",
        "                [chunk],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_len,\n",
        "                truncation=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            outputs = self.model(\n",
        "                tokens[\"input_ids\"],\n",
        "                attention_mask=tokens[\"attention_mask\"],\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            hidden = outputs.hidden_states[-1].to(self.device)  # (1, L, 1280)\n",
        "\n",
        "            # ⭐ CLS TOKEN (position 0)\n",
        "            cls_vec = hidden[:, 0, :].squeeze(0).to(self.device)\n",
        "\n",
        "\n",
        "            embeds.append(cls_vec)\n",
        "\n",
        "        return torch.stack(embeds).mean(0)\n",
        "\n",
        "class AttentionPool(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.att = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, token_embs, mask):\n",
        "        # token_embs: (L, D)\n",
        "        scores = self.att(token_embs).squeeze(-1)     # (L)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)  # ignore PAD tokens\n",
        "        weights = torch.softmax(scores, dim=0).unsqueeze(-1)\n",
        "        return (weights * token_embs).sum(0)\n"
      ],
      "metadata": {
        "id": "1OmV9okLA5wI"
      },
      "id": "1OmV9okLA5wI",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NTEncoderAttention(nn.Module):\n",
        "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        self.pool = AttentionPool(1280).to(device)\n",
        "        self.max_len = self.tokenizer.model_max_length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, seq: str):\n",
        "        seq = seq.upper().replace(\"U\", \"T\")\n",
        "        embeds = []\n",
        "\n",
        "        chunks = [seq[i:i+self.max_len] for i in range(0, len(seq), self.max_len)]\n",
        "\n",
        "        for chunk in chunks:\n",
        "            tokens = self.tokenizer(\n",
        "                [chunk],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_len,\n",
        "                truncation=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            outputs = self.model(\n",
        "                tokens[\"input_ids\"],\n",
        "                attention_mask=tokens[\"attention_mask\"],\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            hidden = outputs.hidden_states[-1].squeeze(0)  # (L, 1280), on CUDA\n",
        "            mask = tokens[\"attention_mask\"].squeeze(0)     # (L), on CUDA\n",
        "\n",
        "            # ⭐ Now pooling is also on CUDA\n",
        "            att_vec = self.pool(hidden, mask).to(self.device)\n",
        "\n",
        "            embeds.append(att_vec)\n",
        "\n",
        "        return torch.stack(embeds).mean(0)\n"
      ],
      "metadata": {
        "id": "AzwMKHsZBDtw"
      },
      "id": "AzwMKHsZBDtw",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_mean = NTEncoderMean(device=device)\n",
        "# encoder_cls  = NTEncoderCLS(device=device)\n",
        "# encoder_att  = NTEncoderAttention(device=device)\n"
      ],
      "metadata": {
        "id": "8-F7ItwdBKwP",
        "outputId": "b3490cb0-d975-4462-ae11-676c927dd2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "d7faa51281564ac091d1eb5f79f4bc40",
            "dbfe331a82cc427489aa5d612e3c2a37",
            "d50f8acfba504096a557c4765a2daea2",
            "c74970f0cced404988d69ccaeedb6aac",
            "8425486da51f41e29afa91d5f04fb827",
            "6d79a54d54b34cb080204f843655da89",
            "a90517511910457c999fc5557e76c5f2",
            "b80d88e6629c42e0bcd92b31cf069049",
            "333379ade17547f693f744fce80f2442",
            "35233705e81646448e4e10959c4413fe",
            "c0cb8d0670b64ccfb1c95d82306b2bf8"
          ]
        }
      },
      "id": "8-F7ItwdBKwP",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/396 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7faa51281564ac091d1eb5f79f4bc40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EsmForMaskedLM LOAD REPORT from: InstaDeepAI/nucleotide-transformer-500m-human-ref\n",
            "Key                         | Status     |  | \n",
            "----------------------------+------------+--+-\n",
            "esm.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to Save embeddings\n",
        "# import pickle\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def cache_tf_embeddings(encoder, tf_seq_dict, save_path=\"tf_embed_cache.pkl\"):\n",
        "#     cache = {}\n",
        "#     print(\"Caching TF embeddings...\")\n",
        "#     for tf_name, seq in tqdm(tf_seq_dict.items()):\n",
        "#         cache[tf_name] = encoder(seq).cpu()\n",
        "#     pickle.dump(cache, open(save_path, \"wb\"))\n",
        "#     print(\"Saved TF embedding cache to\", save_path)\n",
        "\n",
        "\n",
        "# def cache_gene_embeddings(encoder, gene_seq_dict, save_path=\"gene_embed_cache.pkl\"):\n",
        "#     cache = {}\n",
        "#     print(\"Caching Gene embeddings...\")\n",
        "#     for gene_name, seq in tqdm(gene_seq_dict.items()):\n",
        "#         cache[gene_name] = encoder(seq).cpu()\n",
        "#     pickle.dump(cache, open(save_path, \"wb\"))\n",
        "#     print(\"Saved Gene embedding cache to\", save_path)\n",
        "\n",
        "#Function to Save embeddings\n",
        "\n",
        "\n",
        "def ensure_dir(path):\n",
        "    if path != \"\" and not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def cache_tf_embeddings(encoder, tf_seq_dict, save_path=\"./embeds/tf_embed_cache_nt_cls.pkl\"):\n",
        "    # FIX: ensure the directory of save_path exists\n",
        "    ensure_dir(os.path.dirname(save_path))\n",
        "\n",
        "    cache = {}\n",
        "    print(\"Caching TF embeddings...\")\n",
        "\n",
        "    for tf_name, seq in tqdm(tf_seq_dict.items()):\n",
        "        emb = encoder(seq)\n",
        "        if hasattr(emb, \"cpu\"):\n",
        "            emb = emb.cpu()\n",
        "        cache[tf_name] = emb\n",
        "\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(cache, f)\n",
        "\n",
        "    print(f\"Saved TF embedding cache to: {os.path.abspath(save_path)}\")\n",
        "\n",
        "\n",
        "def cache_gene_embeddings(encoder, gene_seq_dict, save_path=\"./embeds/gene_embed_cache_nt_cls.pkl\"):\n",
        "    # FIX: ensure the directory exists\n",
        "    ensure_dir(os.path.dirname(save_path))\n",
        "\n",
        "    cache = {}\n",
        "    print(\"Caching gene embeddings...\")\n",
        "\n",
        "    for gene_name, seq in tqdm(gene_seq_dict.items()):\n",
        "        emb = encoder(seq)\n",
        "        if hasattr(emb, \"cpu\"):\n",
        "            emb = emb.cpu()\n",
        "        cache[gene_name] = emb\n",
        "\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(cache, f)\n",
        "\n",
        "    print(f\"Saved Gene embedding cache to: {os.path.abspath(save_path)}\")\n"
      ],
      "metadata": {
        "id": "5Ho1GrmuGX7I"
      },
      "id": "5Ho1GrmuGX7I",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and Save embeddings\n",
        "# import torch\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load your sequence dictionaries (the same ones the dataloader uses)\n",
        "tf_seq_dict = pickle.load(open(\"tf_sequences.pkl\", \"rb\"))\n",
        "gene_seq_dict = pickle.load(open(\"gene_sequences_4000bp.pkl\", \"rb\"))\n",
        "\n",
        "# Initialize encoder\n",
        "# encoder_mean = NTEncoderMean(device=device)\n",
        "# encoder_cls  = NTEncoderCLS(device=device)\n",
        "encoder_att  = NTEncoderAttention(device=device)\n",
        "\n",
        "# Cache embeddings (takes 3–20 minutes total depending on sizes)\n",
        "# cache_tf_embeddings(encoder, tf_seq_dict)\n",
        "# cache_gene_embeddings(encoder, gene_seq_dict)\n",
        "cache_tf_embeddings(encoder_att, tf_seq_dict, save_path=\"./embeds/tf_attn.pkl\")\n",
        "cache_gene_embeddings(encoder_att, gene_seq_dict, save_path=\"./embeds/gn_attn.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "cd29f72a667e4358b2a64c99c2cb1567",
            "b7660fc60cd740c1877ac9861edb082d",
            "76f6295475ea4b65b1b1bcbe4a875b46",
            "b57bd04e95e741fe8ecb71751d3e4a28",
            "ded32d4359844cd1a942f4dcdb498679",
            "223781576c2545dbbd438d9811b65ed7",
            "0bb259a9d404424fb410975e216aef91",
            "9c8d2987fe8e4cf79fdad648508d19e1",
            "8447b737ca914e4d97db5c5418b5cf5f",
            "590bd756b96544abbb8e4acacb98a334",
            "26b0daedc85c4def8e5f2c8b1430e492"
          ]
        },
        "id": "5NHmpeM-Gbrh",
        "outputId": "619ecfac-f4cd-43ec-9d27-607a6982b550"
      },
      "id": "5NHmpeM-Gbrh",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/396 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd29f72a667e4358b2a64c99c2cb1567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EsmForMaskedLM LOAD REPORT from: InstaDeepAI/nucleotide-transformer-500m-human-ref\n",
            "Key                         | Status     |  | \n",
            "----------------------------+------------+--+-\n",
            "esm.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching TF embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 223/223 [06:46<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved TF embedding cache to: /content/perturbseq-10701/embeds/tf_attn.pkl\n",
            "Caching gene embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5307/5307 [1:46:54<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Gene embedding cache to: /content/perturbseq-10701/embeds/gn_attn.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Download embeddings\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Path to your embedding directory\n",
        "embed_dir = \"./embeds\"\n",
        "\n",
        "# Output zip file name\n",
        "zip_name = \"nt_embedding_caches.zip\"\n",
        "\n",
        "# Create zip\n",
        "shutil.make_archive(\"nt_embedding_caches\", 'zip', embed_dir)\n",
        "\n",
        "# Download zip\n",
        "files.download(zip_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B30rFdV_ONPr",
        "outputId": "e0d7af7e-f760-4c87-93f0-96f06711d150"
      },
      "id": "B30rFdV_ONPr",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_947f0bfe-7fc6-4f47-8470-19f847043d71\", \"nt_embedding_caches.zip\", 54113366)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New dataset/dataloader with embeddings\n",
        "class CachedEmbeddingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, parquet_path, tf_cache_path, gene_cache_path, type=\"train\", train_fraction=0.8, seed=10701):\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "\n",
        "        # load caches\n",
        "        self.tf_cache = pickle.load(open(tf_cache_path, \"rb\"))\n",
        "        self.gene_cache = pickle.load(open(gene_cache_path, \"rb\"))\n",
        "\n",
        "        # remove entries missing from cache\n",
        "        df = df[df[\"tf_name\"].isin(self.tf_cache.keys())]\n",
        "        df = df[df[\"gene_name\"].isin(self.gene_cache.keys())]\n",
        "\n",
        "        # shuffle + split\n",
        "        df = df.sample(frac=1.0, random_state=seed)\n",
        "        n = int(train_fraction * len(df))\n",
        "        if type == \"train\":\n",
        "            self.df = df.iloc[:n].reset_index(drop=True)\n",
        "        else:\n",
        "            self.df = df.iloc[n:].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        tf_emb = self.tf_cache[row[\"tf_name\"]]     # tensor\n",
        "        gene_emb = self.gene_cache[row[\"gene_name\"]]\n",
        "        y = torch.tensor(row[\"expression\"], dtype=torch.float32)\n",
        "        return tf_emb, gene_emb, y\n"
      ],
      "metadata": {
        "id": "g-kSiJttGiQ2"
      },
      "id": "g-kSiJttGiQ2",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cached_loader(type=\"train\", batch_size=32):\n",
        "    ds = CachedEmbeddingDataset(\n",
        "        parquet_path=\"tf_gene_expression.parquet\",\n",
        "        # tf_cache_path=\"tf_embed_cache.pkl\",\n",
        "        tf_cache_path=\"./embeds/tf_attn.pkl\",\n",
        "        # gene_cache_path=\"gene_embed_cache.pkl\",\n",
        "        gene_cache_path=\"./embeds/gn_attn.pkl\",\n",
        "        type=type\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=(type==\"train\"))\n",
        "\n",
        "# def get_cached_loader(type=\"train\", batch_size=32):\n",
        "#     ds = CachedEmbeddingDataset(\n",
        "#         parquet_path=\"tf_gene_expression.parquet\",\n",
        "#         tf_cache_path=\"./embeds/tf_embed_cache.pkl\",\n",
        "#         gene_cache_path=\"./embeds/gene_embed_cache.pkl\",\n",
        "#         type=type\n",
        "#     )\n",
        "#     return DataLoader(ds, batch_size=batch_size, shuffle=(type==\"train\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "5vXB7dg4GoXn"
      },
      "id": "5vXB7dg4GoXn",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # MLP OLD\n",
        "# class NTBiEncoderFast(nn.Module):\n",
        "#     def __init__(self, emb_dim=1280):\n",
        "#         super().__init__()\n",
        "#         self.mlp = nn.Sequential(\n",
        "#             nn.Linear(emb_dim * 2, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, 1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, tf_embs, gene_embs):\n",
        "#         h = torch.cat([tf_embs, gene_embs], dim=-1)\n",
        "#         return self.mlp(h).squeeze(-1)\n"
      ],
      "metadata": {
        "id": "UcE7cNbKGu8X"
      },
      "id": "UcE7cNbKGu8X",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP new\n",
        "class InteractionMLP(nn.Module):\n",
        "    def __init__(self, emb_dim=1280):\n",
        "        super().__init__()\n",
        "\n",
        "        # TF emb (1280) + gene emb (1280) + interaction (1280)\n",
        "        in_dim = emb_dim * 3\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tf_emb, gene_emb):\n",
        "        interaction = tf_emb * gene_emb\n",
        "        h = torch.cat([tf_emb, gene_emb, interaction], dim=-1)\n",
        "        return self.net(h).squeeze(-1)\n"
      ],
      "metadata": {
        "id": "F7o3eW5iEi3_"
      },
      "id": "F7o3eW5iEi3_",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Training Loop\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch_cached_nt(model, loader, optimizer, mu, sigma, device=\"cuda\"):\n",
        "    model.train()\n",
        "    total_loss, N = 0.0, 0\n",
        "    pbar = tqdm(loader)\n",
        "\n",
        "    for tf_emb, gene_emb, y in pbar:\n",
        "        tf_emb = tf_emb.to(device)\n",
        "        gene_emb = gene_emb.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        preds = model(tf_emb, gene_emb)\n",
        "        loss = weighted_mse_loss(preds, y, mu, sigma)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y)\n",
        "        N += len(y)\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    return total_loss / N\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mclxuPOG7H4Y"
      },
      "id": "mclxuPOG7H4Y",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = get_cached_loader(type=\"train\", batch_size=64)\n",
        "test_loader = get_cached_loader(type=\"test\", batch_size=64)\n",
        "\n",
        "# model_nt = NTBiEncoderFast(emb_dim=1280).to(device)\n",
        "model_nt = InteractionMLP(emb_dim=1280).to(device)\n",
        "optimizer = torch.optim.Adam(model_nt.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(5):\n",
        "    loss = train_one_epoch_cached_nt(model_nt, train_loader, optimizer, mu, sigma)\n",
        "    print(\"Epoch\", epoch, \"Loss\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlsNbvwd7H5Q",
        "outputId": "e5fb0754-11e7-4364-d263-712bf4c58cb7"
      },
      "id": "ZlsNbvwd7H5Q",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14794/14794 [02:47<00:00, 88.51it/s, loss=0.0118]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss 0.04422615736868941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14794/14794 [02:46<00:00, 89.03it/s, loss=0.0448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 0.04438143329478897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14794/14794 [02:47<00:00, 88.35it/s, loss=0.0303]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss 0.0442378659344249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14794/14794 [02:49<00:00, 87.28it/s, loss=0.0236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss 0.04405613111517244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14794/14794 [02:49<00:00, 87.15it/s, loss=0.0141]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss 0.044517392084879974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_nt_cached(model, loader, mu, sigma, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    preds_all, y_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tf_emb, gene_emb, y in loader:\n",
        "            tf_emb = tf_emb.to(device)\n",
        "            gene_emb = gene_emb.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            preds = model(tf_emb, gene_emb)\n",
        "\n",
        "            preds_all.append(preds.cpu())\n",
        "            y_all.append(y.cpu())\n",
        "\n",
        "    preds_all = torch.cat(preds_all)\n",
        "    y_all = torch.cat(y_all)\n",
        "\n",
        "    mse = ((preds_all - y_all)**2).mean().item()\n",
        "    corr = torch.corrcoef(torch.stack([preds_all, y_all]))[0, 1].item()\n",
        "\n",
        "    # Large-effect subset\n",
        "    z = (y_all - mu) / sigma\n",
        "    mask = torch.abs(z) > 1.0\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        mse_big = ((preds_all[mask] - y_all[mask])**2).mean().item()\n",
        "        corr_big = torch.corrcoef(torch.stack([preds_all[mask], y_all[mask]]))[0, 1].item()\n",
        "    else:\n",
        "        mse_big, corr_big = None, None\n",
        "\n",
        "    return mse, corr, mse_big, corr_big\n"
      ],
      "metadata": {
        "id": "Vc2Xl9hv7LqY"
      },
      "id": "Vc2Xl9hv7LqY",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse, corr, mse_big, corr_big = evaluate_nt_cached(model_nt, test_loader, mu, sigma, device=device)\n",
        "\n",
        "print(\"=== Evaluation Results ===\")\n",
        "print(f\"Test MSE:          {mse:.6f}\")\n",
        "print(f\"Test Corr:         {corr:.4f}\")\n",
        "print(f\"Big-Effect MSE:    {mse_big:.6f}\")\n",
        "print(f\"Big-Effect Corr:   {corr_big:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8AcUr0SivIz",
        "outputId": "0d87d83b-1e02-4c98-97c4-211a2302dd32"
      },
      "id": "A8AcUr0SivIz",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation Results ===\n",
            "Test MSE:          0.020634\n",
            "Test Corr:         0.0997\n",
            "Big-Effect MSE:    0.093415\n",
            "Big-Effect Corr:   0.1613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_nt.state_dict(), \"nt_attn_int_model.pt\")\n",
        "print(\"Model saved as nt_attn_int_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GNCtbcRmr2x",
        "outputId": "3b394180-2d2a-430a-9139-cba06f61ecd7"
      },
      "id": "_GNCtbcRmr2x",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as nt_attn_int_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"nt_attn_int_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "boNN8ZnrmsfY",
        "outputId": "049c2486-94f6-4c8a-d998-cc4b95da55f3"
      },
      "id": "boNN8ZnrmsfY",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_109dbba5-ec67-42e9-87ba-acfecfff2e47\", \"nt_attn_int_model.pt\", 42225061)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GwkgM8Fup1Ie"
      },
      "id": "GwkgM8Fup1Ie",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7faa51281564ac091d1eb5f79f4bc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbfe331a82cc427489aa5d612e3c2a37",
              "IPY_MODEL_d50f8acfba504096a557c4765a2daea2",
              "IPY_MODEL_c74970f0cced404988d69ccaeedb6aac"
            ],
            "layout": "IPY_MODEL_8425486da51f41e29afa91d5f04fb827"
          }
        },
        "dbfe331a82cc427489aa5d612e3c2a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d79a54d54b34cb080204f843655da89",
            "placeholder": "​",
            "style": "IPY_MODEL_a90517511910457c999fc5557e76c5f2",
            "value": "Loading weights: 100%"
          }
        },
        "d50f8acfba504096a557c4765a2daea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80d88e6629c42e0bcd92b31cf069049",
            "max": 396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_333379ade17547f693f744fce80f2442",
            "value": 396
          }
        },
        "c74970f0cced404988d69ccaeedb6aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35233705e81646448e4e10959c4413fe",
            "placeholder": "​",
            "style": "IPY_MODEL_c0cb8d0670b64ccfb1c95d82306b2bf8",
            "value": " 396/396 [00:00&lt;00:00, 1140.64it/s, Materializing param=lm_head.layer_norm.weight]"
          }
        },
        "8425486da51f41e29afa91d5f04fb827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d79a54d54b34cb080204f843655da89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a90517511910457c999fc5557e76c5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b80d88e6629c42e0bcd92b31cf069049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333379ade17547f693f744fce80f2442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35233705e81646448e4e10959c4413fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0cb8d0670b64ccfb1c95d82306b2bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd29f72a667e4358b2a64c99c2cb1567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7660fc60cd740c1877ac9861edb082d",
              "IPY_MODEL_76f6295475ea4b65b1b1bcbe4a875b46",
              "IPY_MODEL_b57bd04e95e741fe8ecb71751d3e4a28"
            ],
            "layout": "IPY_MODEL_ded32d4359844cd1a942f4dcdb498679"
          }
        },
        "b7660fc60cd740c1877ac9861edb082d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223781576c2545dbbd438d9811b65ed7",
            "placeholder": "​",
            "style": "IPY_MODEL_0bb259a9d404424fb410975e216aef91",
            "value": "Loading weights: 100%"
          }
        },
        "76f6295475ea4b65b1b1bcbe4a875b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c8d2987fe8e4cf79fdad648508d19e1",
            "max": 396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8447b737ca914e4d97db5c5418b5cf5f",
            "value": 396
          }
        },
        "b57bd04e95e741fe8ecb71751d3e4a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590bd756b96544abbb8e4acacb98a334",
            "placeholder": "​",
            "style": "IPY_MODEL_26b0daedc85c4def8e5f2c8b1430e492",
            "value": " 396/396 [00:00&lt;00:00, 1139.52it/s, Materializing param=lm_head.layer_norm.weight]"
          }
        },
        "ded32d4359844cd1a942f4dcdb498679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223781576c2545dbbd438d9811b65ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb259a9d404424fb410975e216aef91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c8d2987fe8e4cf79fdad648508d19e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8447b737ca914e4d97db5c5418b5cf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "590bd756b96544abbb8e4acacb98a334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b0daedc85c4def8e5f2c8b1430e492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}