{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navidh86/perturbseq-10701/blob/master/nt_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyG_N0YZoyDa",
        "outputId": "0d1977c3-f353-4f5a-ac58-f87fb9b6b17f"
      },
      "outputs": [],
      "source": [
        "# # ONLY FOR COLAB\n",
        "# !git clone https://github.com/navidh86/perturbseq-10701.git\n",
        "# %cd ./perturbseq-10701\n",
        "# !pip install fastparquet tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96_JcF_xpCQw",
        "outputId": "84b7f130-fc44-436d-f2bd-d8e81f92fde1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install --upgrade git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OB69xvlgpKcB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\navid\\anaconda3\\envs\\torchgpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from enformer_pytorch import Enformer, seq_indices_to_one_hot\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73I8yRgzpZwo",
        "outputId": "ce9d9097-a64b-4c71-d62a-2a082314d602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 11051\n",
            "Test size:  2764\n"
          ]
        }
      ],
      "source": [
        "from reference_data_classification import (\n",
        "    PairPerturbSeqDataset,\n",
        "    perturbseq_collate_2,\n",
        "    get_dataloader\n",
        ")\n",
        "\n",
        "train_loader = get_dataloader(\n",
        "    parquet_path=\"tf_gene_expression_labeled_v2.parquet\",\n",
        "    tf_sequences_path=\"tf_sequences.pkl\",\n",
        "    gene_sequences_path=\"gene_sequences_4000bp.pkl\",\n",
        "    batch_size=32,\n",
        "    type=\"train\",\n",
        "    majority_fraction=0.005\n",
        ")\n",
        "\n",
        "test_loader = get_dataloader(\n",
        "    parquet_path=\"tf_gene_expression_labeled_v2.parquet\",\n",
        "    tf_sequences_path=\"tf_sequences.pkl\",\n",
        "    gene_sequences_path=\"gene_sequences_4000bp.pkl\",\n",
        "    batch_size=32,\n",
        "    type=\"test\",\n",
        "    majority_fraction=0.005\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_loader.dataset))\n",
        "print(\"Test size: \", len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "expression_label",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "count",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "feb97339-d331-47ff-ba91-640624892dfa",
              "rows": [
                [
                  "2",
                  "4415"
                ],
                [
                  "1",
                  "4010"
                ],
                [
                  "0",
                  "2626"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 3
              }
            },
            "text/plain": [
              "expression_label\n",
              "2    4415\n",
              "1    4010\n",
              "0    2626\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader.dataset.df['expression_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qMY0NZ7u9jD",
        "outputId": "837c1702-edf5-46ba-c503-d9f9e4038848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF embedding count: 223\n",
            "Gene embedding count: 5307\n",
            "TF embedding dim: torch.Size([5313])\n",
            "Gene embedding dim: torch.Size([5313])\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "# Load cached embeddings\n",
        "tf_embed_cache = pickle.load(open(\"./embeds/tf_enformer_alternate.pkl\", \"rb\"))\n",
        "gene_embed_cache = pickle.load(open(\"./embeds/gn_enformer_alternate.pkl\", \"rb\"))\n",
        "\n",
        "# Convert all embeddings to torch tensors (if stored as numpy)\n",
        "for k in tf_embed_cache:\n",
        "    if not isinstance(tf_embed_cache[k], torch.Tensor):\n",
        "        tf_embed_cache[k] = torch.tensor(tf_embed_cache[k], dtype=torch.float32)\n",
        "\n",
        "for k in gene_embed_cache:\n",
        "    if not isinstance(gene_embed_cache[k], torch.Tensor):\n",
        "        gene_embed_cache[k] = torch.tensor(gene_embed_cache[k], dtype=torch.float32)\n",
        "\n",
        "# Inspect shapes\n",
        "first_tf = next(iter(tf_embed_cache.values()))\n",
        "first_gene = next(iter(gene_embed_cache.values()))\n",
        "\n",
        "print(\"TF embedding count:\", len(tf_embed_cache))\n",
        "print(\"Gene embedding count:\", len(gene_embed_cache))\n",
        "print(\"TF embedding dim:\", first_tf.shape)\n",
        "print(\"Gene embedding dim:\", first_gene.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bldOROT3pcHf"
      },
      "outputs": [],
      "source": [
        "class InteractionMLP(nn.Module):\n",
        "    def __init__(self, tf_embed_cache, gene_embed_cache, hidden_dim=1024, num_classes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tf_embed_cache = tf_embed_cache\n",
        "        self.gene_embed_cache = gene_embed_cache\n",
        "\n",
        "        tf_dim = next(iter(tf_embed_cache.values())).shape[0]\n",
        "        gene_dim = next(iter(gene_embed_cache.values())).shape[0]\n",
        "\n",
        "        in_dim = tf_dim + gene_dim   # 1280 + 1280\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(hidden_dim // 2, 128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, num_classes)   # logits for 3 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        \"\"\"\n",
        "        batch_x = list of dicts:\n",
        "        [\n",
        "           {\"tf_name\": ..., \"gene_name\": ...},\n",
        "           ...\n",
        "        ]\n",
        "        \"\"\"\n",
        "        vectors = []\n",
        "\n",
        "        for item in batch_x:\n",
        "            tf_vec = self.tf_embed_cache[item[\"tf_name\"]]\n",
        "            gene_vec = self.gene_embed_cache[item[\"gene_name\"]]\n",
        "            pair_vec = torch.cat([tf_vec, gene_vec], dim=-1)\n",
        "            vectors.append(pair_vec)\n",
        "\n",
        "        X = torch.stack(vectors).to(device)\n",
        "        return self.net(X)   # shape (batch, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fhYoqDhA0rMz"
      },
      "outputs": [],
      "source": [
        "class InteractionMLP2(nn.Module):\n",
        "    def __init__(self, tf_embed_cache, gene_embed_cache, hidden_dim=1024, num_classes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tf_cache = tf_embed_cache\n",
        "        self.gene_cache = gene_embed_cache\n",
        "\n",
        "        tf_dim = next(iter(tf_embed_cache.values())).shape[0]\n",
        "        gene_dim = next(iter(gene_embed_cache.values())).shape[0]\n",
        "\n",
        "        in_dim = tf_dim + gene_dim + tf_dim  # concat + product\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(hidden_dim//2, 128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        vecs = []\n",
        "        for item in batch_x:\n",
        "            tf = self.tf_cache[item[\"tf_name\"]]\n",
        "            gene = self.gene_cache[item[\"gene_name\"]]\n",
        "            inter = tf * gene  # IMPORTANT\n",
        "            vecs.append(torch.cat([tf, gene, inter], dim=-1))\n",
        "        x = torch.stack(vecs).to(device)\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gCF8Zdvps8fX"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        logits = model(batch_x)  # shape (B, 3)\n",
        "        loss = loss_fn(logits, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_loss += loss.item() * len(batch_y)\n",
        "        total_correct += (preds == batch_y).sum().item()\n",
        "        total_samples += len(batch_y)\n",
        "\n",
        "    return total_loss/total_samples, total_correct/total_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "63H3zaUds-fv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        logits = model(batch_x)\n",
        "        loss = loss_fn(logits, batch_y)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        total_loss += loss.item() * len(batch_y)\n",
        "        total_correct += (preds == batch_y).sum().item()\n",
        "        total_samples += len(batch_y)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    return avg_loss, accuracy, macro_f1, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BieWsT4es_tX"
      },
      "outputs": [],
      "source": [
        "model = InteractionMLP(\n",
        "    tf_embed_cache=tf_embed_cache,\n",
        "    gene_embed_cache=gene_embed_cache\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw7irBwStEpo",
        "outputId": "835bd97a-dd50-4e3b-e627-337021dffa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train Loss: 1.0792, Train Acc: 0.3983 | Test Loss: 1.0753, Test Acc: 0.4092, Test F1: 0.3083\n",
            "Epoch 02 | Train Loss: 1.0748, Train Acc: 0.3972 | Test Loss: 1.0743, Test Acc: 0.3994, Test F1: 0.1903\n",
            "Epoch 03 | Train Loss: 1.0711, Train Acc: 0.4002 | Test Loss: 1.0677, Test Acc: 0.4016, Test F1: 0.1969\n",
            "Epoch 04 | Train Loss: 1.0636, Train Acc: 0.4129 | Test Loss: 1.0558, Test Acc: 0.4334, Test F1: 0.3464\n",
            "Epoch 05 | Train Loss: 1.0454, Train Acc: 0.4370 | Test Loss: 1.0251, Test Acc: 0.4645, Test F1: 0.4266\n",
            "Epoch 06 | Train Loss: 1.0278, Train Acc: 0.4567 | Test Loss: 1.0049, Test Acc: 0.4674, Test F1: 0.4281\n",
            "Epoch 07 | Train Loss: 1.0268, Train Acc: 0.4496 | Test Loss: 1.0160, Test Acc: 0.4555, Test F1: 0.4047\n",
            "Epoch 08 | Train Loss: 1.0130, Train Acc: 0.4647 | Test Loss: 1.0123, Test Acc: 0.4928, Test F1: 0.4073\n",
            "Epoch 09 | Train Loss: 1.0051, Train Acc: 0.4749 | Test Loss: 0.9877, Test Acc: 0.4993, Test F1: 0.4549\n",
            "Epoch 10 | Train Loss: 0.9970, Train Acc: 0.4831 | Test Loss: 0.9837, Test Acc: 0.4823, Test F1: 0.4686\n",
            "Epoch 11 | Train Loss: 0.9917, Train Acc: 0.4917 | Test Loss: 0.9740, Test Acc: 0.4920, Test F1: 0.4666\n",
            "Epoch 12 | Train Loss: 0.9909, Train Acc: 0.4909 | Test Loss: 0.9822, Test Acc: 0.5145, Test F1: 0.4937\n",
            "Epoch 13 | Train Loss: 0.9906, Train Acc: 0.4844 | Test Loss: 0.9819, Test Acc: 0.5159, Test F1: 0.4336\n",
            "Epoch 14 | Train Loss: 0.9877, Train Acc: 0.4893 | Test Loss: 0.9735, Test Acc: 0.4978, Test F1: 0.4555\n",
            "Epoch 15 | Train Loss: 0.9852, Train Acc: 0.4916 | Test Loss: 0.9643, Test Acc: 0.5051, Test F1: 0.4292\n",
            "Epoch 16 | Train Loss: 0.9884, Train Acc: 0.4849 | Test Loss: 0.9595, Test Acc: 0.5141, Test F1: 0.4661\n",
            "Epoch 17 | Train Loss: 0.9820, Train Acc: 0.4968 | Test Loss: 0.9651, Test Acc: 0.5130, Test F1: 0.5087\n",
            "Epoch 18 | Train Loss: 0.9813, Train Acc: 0.4968 | Test Loss: 0.9756, Test Acc: 0.5188, Test F1: 0.4832\n",
            "Epoch 19 | Train Loss: 0.9772, Train Acc: 0.4974 | Test Loss: 0.9532, Test Acc: 0.5373, Test F1: 0.5148\n",
            "Epoch 20 | Train Loss: 0.9766, Train Acc: 0.4997 | Test Loss: 0.9656, Test Acc: 0.5434, Test F1: 0.5245\n",
            "Epoch 21 | Train Loss: 0.9664, Train Acc: 0.5086 | Test Loss: 0.9483, Test Acc: 0.5101, Test F1: 0.4659\n",
            "Epoch 22 | Train Loss: 0.9797, Train Acc: 0.4960 | Test Loss: 0.9660, Test Acc: 0.5163, Test F1: 0.4904\n",
            "Epoch 23 | Train Loss: 0.9823, Train Acc: 0.4824 | Test Loss: 0.9461, Test Acc: 0.5293, Test F1: 0.4516\n",
            "Epoch 24 | Train Loss: 0.9747, Train Acc: 0.4948 | Test Loss: 0.9675, Test Acc: 0.5185, Test F1: 0.4182\n",
            "Epoch 25 | Train Loss: 0.9740, Train Acc: 0.4996 | Test Loss: 0.9406, Test Acc: 0.5326, Test F1: 0.4953\n",
            "Epoch 26 | Train Loss: 0.9695, Train Acc: 0.4959 | Test Loss: 0.9528, Test Acc: 0.5297, Test F1: 0.4756\n",
            "Epoch 27 | Train Loss: 0.9760, Train Acc: 0.5008 | Test Loss: 0.9345, Test Acc: 0.5438, Test F1: 0.5098\n",
            "Epoch 28 | Train Loss: 0.9761, Train Acc: 0.5044 | Test Loss: 0.9357, Test Acc: 0.5384, Test F1: 0.4725\n",
            "Epoch 29 | Train Loss: 0.9860, Train Acc: 0.4876 | Test Loss: 0.9384, Test Acc: 0.5224, Test F1: 0.4826\n",
            "Epoch 30 | Train Loss: 0.9644, Train Acc: 0.5057 | Test Loss: 0.9528, Test Acc: 0.5289, Test F1: 0.4418\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 31):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
        "    test_loss, test_acc, test_f1, _, _ = eval_model(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM3VCLqvCN3",
        "outputId": "706ff8e1-199b-42cc-f585-87efdbe7f2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.5289435600578871\n",
            "Final Test Macro F1: 0.44182153567152554\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6224    0.0928    0.1616       657\n",
            "           1     0.5120    0.8285    0.6329      1003\n",
            "           2     0.5465    0.5163    0.5310      1104\n",
            "\n",
            "    accuracy                         0.5289      2764\n",
            "   macro avg     0.5603    0.4792    0.4418      2764\n",
            "weighted avg     0.5520    0.5289    0.4802      2764\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc, test_f1, y_true, y_pred = eval_model(model, test_loader)\n",
        "\n",
        "print(\"Final Test Accuracy:\", test_acc)\n",
        "print(\"Final Test Macro F1:\", test_f1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNjClJPnvYPU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNXdctc4hCjFJVN/j7535Vt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torchgpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
