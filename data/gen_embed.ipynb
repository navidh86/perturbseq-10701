{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd18272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Only\n",
    "!git clone https://github.com/navidh86/perturbseq-10701.git\n",
    "%cd ./perturbseq-10701\n",
    "!pip install fastparquet tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Only\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.att = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, token_embs, mask):\n",
    "        # token_embs: (L, D)\n",
    "        scores = self.att(token_embs).squeeze(-1)     # (L)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)  # ignore PAD tokens\n",
    "        weights = torch.softmax(scores, dim=0).unsqueeze(-1)\n",
    "        return (weights * token_embs).sum(0)\n",
    "\n",
    "class NTEncoderCLS(nn.Module):\n",
    "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.max_len = self.tokenizer.model_max_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, seq: str):\n",
    "        seq = seq.upper().replace(\"U\", \"T\")\n",
    "        embeds = []\n",
    "\n",
    "        chunks = [seq[i:i+self.max_len] for i in range(0, len(seq), self.max_len)]\n",
    "\n",
    "        for chunk in chunks:\n",
    "            tokens = self.tokenizer(\n",
    "                [chunk],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_len,\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                tokens[\"input_ids\"],\n",
    "                attention_mask=tokens[\"attention_mask\"],\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "            hidden = outputs.hidden_states[-1].to(self.device)  # (1, L, D)\n",
    "\n",
    "            # CLS TOKEN (position 0)\n",
    "            cls_vec = hidden[:, 0, :].squeeze(0).to(self.device)\n",
    "\n",
    "            embeds.append(cls_vec)\n",
    "\n",
    "        return torch.stack(embeds).mean(0)\n",
    "\n",
    "class NTEncoderAttention(nn.Module):\n",
    "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.pool = AttentionPool(1280).to(device)\n",
    "        self.max_len = self.tokenizer.model_max_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, seq: str):\n",
    "        seq = seq.upper().replace(\"U\", \"T\")\n",
    "        embeds = []\n",
    "\n",
    "        chunks = [seq[i:i+self.max_len] for i in range(0, len(seq), self.max_len)]\n",
    "\n",
    "        for chunk in chunks:\n",
    "            tokens = self.tokenizer(\n",
    "                [chunk],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_len,\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                tokens[\"input_ids\"],\n",
    "                attention_mask=tokens[\"attention_mask\"],\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "            hidden = outputs.hidden_states[-1].squeeze(0)  # (L, 1280)\n",
    "            mask = tokens[\"attention_mask\"].squeeze(0)     # (L)\n",
    "\n",
    "            att_vec = self.pool(hidden, mask).to(self.device)\n",
    "\n",
    "            embeds.append(att_vec)\n",
    "\n",
    "        return torch.stack(embeds).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    if path != \"\" and not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def cache_tf_embeddings(encoder, tf_seq_dict, save_path=\"../embeds/tf_cls.pkl\"):\n",
    "    ensure_dir(os.path.dirname(save_path))\n",
    "    cache = {}\n",
    "    print(\"Caching TF embeddings...\")\n",
    "    for tf_name, seq in tqdm(tf_seq_dict.items()):\n",
    "        emb = encoder(seq)\n",
    "        if hasattr(emb, \"cpu\"):\n",
    "            emb = emb.cpu()\n",
    "        cache[tf_name] = emb\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "    print(f\"Saved TF embedding cache to: {os.path.abspath(save_path)}\")\n",
    "\n",
    "def cache_gene_embeddings(encoder, gene_seq_dict, save_path=\"../embeds/gn_cls.pkl\"):\n",
    "    ensure_dir(os.path.dirname(save_path))\n",
    "    cache = {}\n",
    "    print(\"Caching gene embeddings...\")\n",
    "    for gene_name, seq in tqdm(gene_seq_dict.items()):\n",
    "        emb = encoder(seq)\n",
    "        if hasattr(emb, \"cpu\"):\n",
    "            emb = emb.cpu()\n",
    "        cache[gene_name] = emb\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "    print(f\"Saved Gene embedding cache to: {os.path.abspath(save_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence dictionaries (same format as your dataloader expects)\n",
    "tf_seq_dict = pickle.load(open(\"data/tf_sequences.pkl\", \"rb\"))\n",
    "gene_seq_dict = pickle.load(open(\"data/gene_sequences_4000bp.pkl\", \"rb\"))\n",
    "\n",
    "# Initialize encoder (choose CLS or Attention)\n",
    "encoder_cls = NTEncoderCLS(device=device)\n",
    "# encoder_att = NTEncoderAttention(device=device)\n",
    "\n",
    "# Cache embeddings\n",
    "cache_tf_embeddings(encoder_cls, tf_seq_dict, save_path=\"../embeds/tf_cls.pkl\")\n",
    "cache_gene_embeddings(encoder_cls, gene_seq_dict, save_path=\"../embeds/gn_cls.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
